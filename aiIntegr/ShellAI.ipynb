{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91de256a",
   "metadata": {},
   "source": [
    "# ShellAI – Natural Language Interfaces for the Custom Shell\n",
    "\n",
    "This notebook explores the idea of using **AI models** to accept natural language input in your shell and translate it into concrete commands that your existing shell and command modules can execute.\n",
    "\n",
    "It is **not** about model training in depth or API keys, but about:\n",
    "- The *idea* of AI-assisted shell interaction.\n",
    "- Typical model choices and their size/constraints.\n",
    "- How to leverage your existing **command anatomy** (`cmd_spec_t`, `argtable3`, docs) as training or retrieval data.\n",
    "- Design patterns (fine-tuning vs RAG) that keep the AI suggestions aligned with what your shell actually supports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec7e8d",
   "metadata": {},
   "source": [
    "## 1. Motivation and high-level idea\n",
    "\n",
    "Your course shell is already powerful:\n",
    "- It has a grammar (BNFC shell lab) for pipelines, redirection, background jobs.\n",
    "- It has a growing library of commands, each implemented with a standard anatomy:\n",
    "  - `cmd_spec_t` (name, summary, long_help).\n",
    "  - Argument parsing and `--help` via `argtable3`.\n",
    "  - Registration in a command registry.\n",
    "\n",
    "**ShellAI** asks: can we add a **natural language front-end** on top of this?\n",
    "\n",
    "Example:\n",
    "\n",
    "- User types:\n",
    "\n",
    "  ```text\n",
    "  @show me all `.c` files modified today\n",
    "  ```\n",
    "\n",
    "- AI helper returns:\n",
    "\n",
    "  ```text\n",
    "  ls -lt --time-style=+%Y-%m-%d *.c\n",
    "  ```\n",
    "\n",
    "- Shell prints:\n",
    "\n",
    "  ```text\n",
    "  Suggested: ls -lt --time-style=+%Y-%m-%d *.c\n",
    "  Run this? (y/n)\n",
    "  ```\n",
    "\n",
    "- If the user accepts, the shell feeds that line into the normal parse → execute pipeline.\n",
    "\n",
    "The key property: **the AI never runs commands directly**. It only suggests a string; the shell remains responsible for parsing, checking, and executing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dc412",
   "metadata": {},
   "source": [
    "## 2. Architecture: AI-assisted shell as a tool user\n",
    "\n",
    "We treat the AI as a *helper process* that maps natural language → shell commands.\n",
    "\n",
    "Baseline architecture:\n",
    "\n",
    "- Shell (`mysh`):\n",
    "  - REPL and parser (BNFC mini-shell).\n",
    "  - Command registry and built-ins (`cmd_spec_t`, `run`, `print_usage`).\n",
    "  - Integration point for AI:\n",
    "    - If line starts with `@`, send the rest to `mysh_llm`.\n",
    "    - Read one line back: suggested command.\n",
    "    - Ask the user for confirmation and, if accepted, execute.\n",
    "\n",
    "- AI helper (`mysh_llm`):\n",
    "  - Standalone program (could be Python, C, etc.).\n",
    "  - Talks to an AI model (local or remote) using its preferred API.\n",
    "  - Can optionally perform retrieval over your command docs before calling the model.\n",
    "  - Outputs exactly **one line**: a suggested shell command.\n",
    "\n",
    "This separation has several advantages:\n",
    "- You can swap models or implementations without changing the shell.\n",
    "- The shell remains deterministic and testable; the AI is only a suggestion engine.\n",
    "- You can log all `@` queries and suggested commands for debugging and safety.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d3729",
   "metadata": {},
   "source": [
    "## 3. What models exist for CLI-like behavior?\n",
    "\n",
    "Many modern language models have strong familiarity with Unix command lines, even if they are **not dedicated CLI models**.\n",
    "\n",
    "### 3.1 General-purpose large models\n",
    "\n",
    "Examples (sizes and capabilities may change over time):\n",
    "\n",
    "- Proprietary, cloud-hosted:\n",
    "  - GPT-4-class models (OpenAI) – tens to hundreds of billions of parameters (not publicly specified), trained on code and shell usage.\n",
    "  - Claude 3, Gemini, etc. – similar scale and training mix.\n",
    "- Open models:\n",
    "  - LLaMA 3 family (e.g., 8B, 70B parameter variants).\n",
    "  - Mistral / Mixtral (e.g., 7B, 8x7B MoE).\n",
    "  - Other code-focused models (e.g., Code LLaMA, StarCoder-like models).\n",
    "\n",
    "These models generally *already know* common Unix commands and patterns (e.g. `ls`, `grep`, pipes), because their training corpora include code repositories, configuration files, and shell transcripts.\n",
    "\n",
    "### 3.2 CLI-specific or agentic wrappers\n",
    "\n",
    "Most \"shell GPT\" tools are actually **wrappers** around a general model:\n",
    "- They prompt the model with examples of command usage.\n",
    "- They sometimes enforce safety rules (never run `rm -rf /`, etc.).\n",
    "- They may inspect the filesystem or command outputs.\n",
    "\n",
    "Purely CLI-specific small models exist in research, but in practice you are more likely to:\n",
    "- Use a general model + good prompting.\n",
    "- Or fine-tune a mid-size open model (e.g., 7B–13B parameters) with shell-specific data.\n",
    "\n",
    "For teaching, you do **not** need to pick a specific model. Instead, focus on:\n",
    "- How to design prompts.\n",
    "- How to constrain outputs.\n",
    "- How to use your own shell’s knowledge (from the command anatomy) to improve reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80459e59",
   "metadata": {},
   "source": [
    "## 4. Using your command anatomy as knowledge\n",
    "\n",
    "Your command modules already encode **structured knowledge** that an AI could use:\n",
    "\n",
    "- `cmd_spec_t`:\n",
    "  - `name` – the literal command name.\n",
    "  - `summary` – one-line description.\n",
    "  - `long_help` – longer description / Markdown.\n",
    "- `argtable3` definitions:\n",
    "  - Short/long option names.\n",
    "  - Expected value types (file, string, integer, etc.).\n",
    "  - Help strings per option.\n",
    "- Generated docs:\n",
    "  - `--help` or `--help-md` outputs.\n",
    "  - Additional README or example sections.\n",
    "\n",
    "We can exploit this in two main ways:\n",
    "\n",
    "1. **Fine-tuning / transfer learning**: turn this information into a training dataset.\n",
    "2. **Retrieval-Augmented Generation (RAG)**: dynamically feed relevant command docs to the model at query time.\n",
    "\n",
    "Both approaches share a first step: **extract a machine-readable catalog** of your commands.\n",
    "\n",
    "### 4.1 Building a command catalog\n",
    "\n",
    "You can imagine a JSON file like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"commands\": [\n",
    "    {\n",
    "      \"name\": \"hello\",\n",
    "      \"summary\": \"print a friendly greeting\",\n",
    "      \"description\": \"Print a greeting, optionally addressing a specific NAME.\",\n",
    "      \"usage\": \"hello [-h] [-n NAME]\",\n",
    "      \"options\": [\n",
    "        { \"short\": \"-h\", \"long\": \"--help\", \"help\": \"show help and exit\" },\n",
    "        { \"short\": \"-n\", \"long\": \"--name\", \"arg\": \"NAME\", \"help\": \"name to greet\" }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"wc\",\n",
    "      \"summary\": \"print newline, word, and byte counts\", \n",
    "      \"description\": \"Count lines, words, and bytes in files or stdin.\",\n",
    "      \"usage\": \"wc [OPTION]... [FILE]...\",\n",
    "      \"options\": [ /* ... */ ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "You can generate this in several ways:\n",
    "- From `cmd_spec_t` and `argtable3` directly (C-side introspection).\n",
    "- By running each command with `--help-json` or `--help-md` and parsing the output.\n",
    "\n",
    "Once you have such a catalog, you can either:\n",
    "- Use it to generate synthetic training pairs (for fine-tuning).\n",
    "- Index it for retrieval in a RAG setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca7356a",
   "metadata": {},
   "source": [
    "## 5. Strategy A: Fine-tuning on your commands (transfer learning)\n",
    "\n",
    "**Idea:** teach a base model to map natural language to your specific commands by training it on examples derived from your command catalog.\n",
    "\n",
    "### 5.1 Creating training examples\n",
    "\n",
    "For each command and option combination, you can synthesize pairs like:\n",
    "\n",
    "- Input (natural language):\n",
    "\n",
    "  ```text\n",
    "  greet Alice with the hello command\n",
    "  ```\n",
    "\n",
    "- Output (target CLI):\n",
    "\n",
    "  ```text\n",
    "  hello -n Alice\n",
    "  ```\n",
    "\n",
    "Another example:\n",
    "\n",
    "- Input:\n",
    "\n",
    "  ```text\n",
    "  count lines and words in all .c files\n",
    "  ```\n",
    "\n",
    "- Output:\n",
    "\n",
    "  ```text\n",
    "  wc -lw *.c\n",
    "  ```\n",
    "\n",
    "You can generate many such examples by combining:\n",
    "- Command names and summaries.\n",
    "- Option descriptions.\n",
    "- Simple templates for common tasks (\"list\", \"count\", \"search\", etc.).\n",
    "\n",
    "### 5.2 Fine-tuning a model\n",
    "\n",
    "In practice:\n",
    "- Choose a base model, e.g. a 7B–13B open model that you can run on a GPU or cloud service.\n",
    "- Format training data as instruction–response pairs.\n",
    "- Run a small fine-tuning job (LoRA / parameter-efficient fine-tuning) using an open-source framework.\n",
    "\n",
    "From a **course** perspective, you do not need to actually run this; instead, you can:\n",
    "- Discuss the process conceptually.\n",
    "- Show a few example training pairs in this notebook.\n",
    "- Optionally provide a script template for students who want to experiment.\n",
    "\n",
    "Fine-tuning improves fluency and domain-specific behavior, but it is **heavier** to set up and maintain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330bf27",
   "metadata": {},
   "source": [
    "## 6. Strategy B: Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "RAG avoids fine-tuning by giving a general model **fresh context** on each query.\n",
    "\n",
    "### 6.1 High-level RAG loop\n",
    "\n",
    "1. User types an `@` query:\n",
    "\n",
    "   ```text\n",
    "   @list only hidden files in this directory\n",
    "   ```\n",
    "\n",
    "2. Shell passes the natural language text to your AI helper (`mysh_llm`).\n",
    "\n",
    "3. `mysh_llm` performs **retrieval**:\n",
    "   - Uses embeddings to find the most relevant commands and options from your command catalog.\n",
    "   - For example, it might retrieve docs for `ls` and its `-a`/`--all` flag.\n",
    "\n",
    "4. `mysh_llm` constructs a prompt to the base model:\n",
    "\n",
    "   ```text\n",
    "   You are a command suggestion engine for the mysh shell.\n",
    "   You only suggest commands that exist and options that are documented.\n",
    "\n",
    "   Available commands (subset):\n",
    "   - ls: list directory contents.\n",
    "     Usage: ls [OPTION]... [FILE]...\n",
    "     Options:\n",
    "       -a, --all    do not ignore entries starting with '.'\n",
    "\n",
    "   User request: \"list only hidden files in this directory\".\n",
    "\n",
    "   Suggest a single mysh command line:\n",
    "   ```\n",
    "\n",
    "5. The model responds, e.g.:\n",
    "\n",
    "   ```text\n",
    "   ls -d .??*\n",
    "   ```\n",
    "\n",
    "6. `mysh_llm` returns just that line to the shell, which then asks the user to confirm and executes it.\n",
    "\n",
    "### 6.2 Benefits of RAG\n",
    "\n",
    "- You can change or extend your command set without retraining the model.\n",
    "- The model sees **exact** command syntax at runtime, reducing hallucinations.\n",
    "- You can include additional context, like environment variables or examples, in the retrieved docs.\n",
    "\n",
    "From a course perspective, RAG is a nice illustration of:\n",
    "- How to combine **symbolic knowledge** (your commands and options) with an LLM.\n",
    "- How retrieval and prompting can often replace full fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522858b",
   "metadata": {},
   "source": [
    "## 7. Shell integration recap (ties back to BNF_Shell)\n",
    "\n",
    "On the shell side (C code), the integration is intentionally simple and model-agnostic.\n",
    "\n",
    "### 7.1 Shell logic for `@` lines\n",
    "\n",
    "In your BNFC-based shell (see `BNF_Shell` lab), you can implement:\n",
    "\n",
    "```c\n",
    "int handle_llm_line(const char *query) {\n",
    "    // 1. Call external helper: mysh_llm query\n",
    "    // 2. Capture one line of output: suggested command\n",
    "    // 3. Print it and ask user to confirm\n",
    "    // 4. If confirmed, run the existing parse/execute pipeline on that line\n",
    "}\n",
    "```\n",
    "\n",
    "Main loop sketch:\n",
    "\n",
    "```c\n",
    "while (getline(&line, &len, stdin) != -1) {\n",
    "    if (line[0] == '@') {\n",
    "        handle_llm_line(line + 1); // skip '@'\n",
    "        continue;\n",
    "    }\n",
    "    // normal path: parse line via yyparse, then execute_command(...)\n",
    "}\n",
    "```\n",
    "\n",
    "The **ShellAI** notebook focuses on how `mysh_llm` should behave; the BNF_Shell notebook focuses on plumbing this into your C shell.\n",
    "\n",
    "You can start with a dummy `mysh_llm` that ignores the query and prints a hard-coded command.\n",
    "Once the plumbing works, you or your students can experiment with real models and RAG strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdce30",
   "metadata": {},
   "source": [
    "## 8. Extra-credit assignment outline\n",
    "\n",
    "For students interested in ShellAI, an extra-credit assignment could be:\n",
    "\n",
    "1. **Catalog extraction** (low effort, high value):\n",
    "   - Write a script or small C program that runs each command with `--help-md` or `--help-json`.\n",
    "   - Collect the output into a JSON catalog of commands and options.\n",
    "\n",
    "2. **Prototype RAG helper**:\n",
    "   - In Python, build a simple `mysh_llm` script that:\n",
    "     - Loads the command catalog.\n",
    "     - For each `@` query, picks a likely command by keyword matching (no embeddings needed to start).\n",
    "     - Builds a prompt showing that command’s usage and options.\n",
    "     - Calls an LLM API to generate a suggested command line.\n",
    "     - Prints just that line.\n",
    "\n",
    "3. **Shell integration**:\n",
    "   - Modify your shell’s main loop so that `@` lines go through `handle_llm_line`.\n",
    "   - Confirm that suggested commands are run only after user confirmation.\n",
    "\n",
    "4. **Reflection**:\n",
    "   - Write a short note on:\n",
    "     - Where the model did well.\n",
    "     - Where it hallucinated options or commands.\n",
    "     - How RAG or better prompts improved robustness.\n",
    "\n",
    "This assignment keeps the focus on **systems concepts** (processes, pipes, registries, and structured metadata) while giving a realistic glimpse of how AI-powered developer tools are built.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821a9cf",
   "metadata": {},
   "source": [
    "## 9. Example helper script: `mysh_llm.py`\n",
    "\n",
    "This repository includes a **partly implemented helper script**:\n",
    "\n",
    "- `ShellAI/mysh_llm.py`\n",
    "\n",
    "Its purpose:\n",
    "- Read a natural-language query from the command line.\n",
    "- Ask the shell (or a local JSON file) for the current command catalog.\n",
    "- Build a prompt that lists relevant commands and options.\n",
    "- Optionally call an OpenAI model to get a suggested command line.\n",
    "- Fall back to a simple heuristic if no model is configured.\n",
    "- Print exactly **one line**: the suggested command.\n",
    "\n",
    "### 9.1 Command catalog protocol (simple MCP-like pattern)\n",
    "\n",
    "The script first tries to retrieve a catalog via:\n",
    "\n",
    "```bash\n",
    "mysh --commands-json\n",
    "```\n",
    "\n",
    "Expected JSON structure (example):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"commands\": [\n",
    "    {\n",
    "      \"name\": \"hello\",\n",
    "      \"summary\": \"print a friendly greeting\",\n",
    "      \"description\": \"Print a greeting, optionally addressing a specific NAME.\",\n",
    "      \"usage\": \"hello [-h] [-n NAME]\",\n",
    "      \"options\": [\n",
    "        { \"short\": \"-h\", \"long\": \"--help\", \"arg\": null, \"help\": \"show help and exit\" },\n",
    "        { \"short\": \"-n\", \"long\": \"--name\", \"arg\": \"NAME\", \"help\": \"name to greet\" }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "If this command is not available, `mysh_llm.py` falls back to a local file `commands.json` with the same structure.\n",
    "\n",
    "This pattern is conceptually similar to **Model Context Protocol (MCP)**:\n",
    "- The shell plays the role of a **server** exposing structured knowledge (commands, options, docs).\n",
    "- The helper script is a **client** that fetches that knowledge and feeds it to a model.\n",
    "\n",
    "For this course, we use a very simple custom protocol (just a JSON command) instead of a full MCP implementation.\n",
    "\n",
    "### 9.2 Using `mysh_llm.py` from the shell\n",
    "\n",
    "Once integrated, your shell’s `handle_llm_line` can:\n",
    "\n",
    "```c\n",
    "int handle_llm_line(const char *query) {\n",
    "    // Launch mysh_llm and capture its stdout, e.g. via popen() or pipes.\n",
    "    // Command: mysh_llm \"<query>\"\n",
    "    // Read exactly one line: the suggested CLI (e.g., \"ls -lt *.c\").\n",
    "    // Print it, ask the user to confirm, and if yes, feed it back into yyparse/execute.\n",
    "}\n",
    "```\n",
    "\n",
    "Standalone usage from a terminal:\n",
    "\n",
    "```bash\n",
    "cd ShellAI\n",
    "python mysh_llm.py \"list all C files\"\n",
    "```\n",
    "\n",
    "If you do **not** set `OPENAI_API_KEY`, the script will:\n",
    "- Log (optionally) to stderr that no real model is configured.\n",
    "- Use a simple heuristic to pick a plausible command from the catalog.\n",
    "\n",
    "If you configure the OpenAI client:\n",
    "- `OPENAI_API_KEY` environment variable.\n",
    "- Optionally `MYSH_LLM_MODEL` (default `gpt-4o-mini`).\n",
    "\n",
    "then `mysh_llm.py` will:\n",
    "- Use the command catalog as RAG context.\n",
    "- Ask the model for a single-line suggestion.\n",
    "- Return that suggestion to the shell.\n",
    "\n",
    "This gives you a concrete starting point to experiment with **RAG-based ShellAI** without having to implement all the pieces from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88437e",
   "metadata": {},
   "source": [
    "## 10. Assignment: Implement and integrate `mysh_llm.py`\n",
    "\n",
    "For this **ShellAI** part of the course, the main deliverable is a working `mysh_llm.py` helper and its integration with your shell.\n",
    "\n",
    "### 10.1 Learning goals\n",
    "\n",
    "By doing this assignment, you will:\n",
    "- Design a simple **protocol** between your shell and an AI helper (JSON catalog, `@` lines).\n",
    "- Implement a **RAG-style client** that uses your own command metadata instead of letting the model guess.\n",
    "- Practice using an AI coding assistant as a collaborator, not a replacement:\n",
    "  - You specify interfaces and constraints.\n",
    "  - The AI helps fill in boilerplate and refine code.\n",
    "\n",
    "### 10.2 What you should implement\n",
    "\n",
    "1. **Command catalog output in your shell** (ties back to the BNF_Shell lab):\n",
    "   - Add a built-in or flag, e.g. `mysh --commands-json`, that prints a JSON catalog of available commands.\n",
    "   - At minimum, include for each command:\n",
    "     - `name`\n",
    "     - `summary`\n",
    "     - `description` (can reuse `long_help` or `summary`)\n",
    "   - Extra credit: add `usage` and `options` based on your `argtable3` definitions.\n",
    "\n",
    "2. **Complete or adapt `mysh_llm.py`:**\n",
    "   - Use or extend `ShellAI/mysh_llm.py` as a starting point.\n",
    "   - Ensure it:\n",
    "     - Calls your shell catalog command (or reads `commands.json`).\n",
    "     - Parses the JSON into Python structures.\n",
    "     - Selects relevant commands using a simple scoring or keyword match.\n",
    "     - Builds a prompt that lists those commands and options.\n",
    "     - Either:\n",
    "       - Uses a real LLM (if you configure `OPENAI_API_KEY`), or\n",
    "       - Uses a heuristic fallback (no network).\n",
    "     - Prints exactly **one shell command line**.\n",
    "\n",
    "3. **Shell integration (`@` lines):**\n",
    "   - In your BNFC-based shell:\n",
    "     - Detect when an input line begins with `@`.\n",
    "     - Strip the `@` and pass the rest to `mysh_llm` via a helper like `handle_llm_line`.\n",
    "     - Capture the suggested command line.\n",
    "     - Show it to the user and ask for confirmation.\n",
    "     - If confirmed, feed the suggested line back into your normal parse → execute pipeline.\n",
    "\n",
    "You do **not** have to use a real cloud model to get credit; a heuristic-only `mysh_llm.py` that uses your command catalog is acceptable and already illustrates the integration.\n",
    "\n",
    "### 10.3 Suggested AI prompt for implementing `mysh_llm.py`\n",
    "\n",
    "You are encouraged to use your favorite AI coding assistant when implementing this helper. Below is a suggested starting prompt you can paste into your AI tool and adapt:\n",
    "\n",
    "```text\n",
    "You are my Python systems programming assistant.\n",
    "\n",
    "Context:\n",
    "- I am building a custom Unix-like shell called `mysh` for a course.\n",
    "- Commands are implemented with a standard anatomy (`cmd_spec_t`, argtable3) and registered in a command registry.\n",
    "- The shell will expose a JSON catalog of commands via a flag:\n",
    "\n",
    "    mysh --commands-json\n",
    "\n",
    "  The output is a JSON object like:\n",
    "\n",
    "    {\n",
    "      \"commands\": [\n",
    "        {\n",
    "          \"name\": \"hello\",\n",
    "          \"summary\": \"print a friendly greeting\",\n",
    "          \"description\": \"Print a greeting, optionally addressing a specific NAME.\",\n",
    "          \"usage\": \"hello [-h] [-n NAME]\",\n",
    "          \"options\": [\n",
    "            { \"short\": \"-h\", \"long\": \"--help\", \"arg\": null, \"help\": \"show help and exit\" },\n",
    "            { \"short\": \"-n\", \"long\": \"--name\", \"arg\": \"NAME\", \"help\": \"name to greet\" }\n",
    "          ]\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    }\n",
    "\n",
    "Goal:\n",
    "- Implement a Python script `mysh_llm.py` that the shell will call when a line starts with `@`.\n",
    "- The script should:\n",
    "  1) Read a natural language query from argv[1].\n",
    "  2) Retrieve the command catalog, preferably by running `mysh --commands-json`.\n",
    "  3) Parse the JSON into Python data structures.\n",
    "  4) Select a small subset of relevant commands based on keyword overlap with the query.\n",
    "  5) Build a text prompt that lists those commands and options.\n",
    "  6) EITHER:\n",
    "     - Call an LLM API (if `OPENAI_API_KEY` is set) to get a suggested command line, OR\n",
    "     - Use a simple heuristic fallback (e.g., pick the best command name).\n",
    "  7) Print exactly ONE line to stdout: the suggested shell command line, with no extra text.\n",
    "\n",
    "Constraints:\n",
    "- Use standard Python 3 and the standard library only, except for the optional LLM call.\n",
    "- For the LLM call, assume I may have `openai` installed and an `OPENAI_API_KEY` set; if not, gracefully skip and use the heuristic.\n",
    "- Include clear functions for:\n",
    "  - Loading the catalog from the shell or a local JSON file.\n",
    "  - Scoring commands relative to the query.\n",
    "  - Building the prompt.\n",
    "  - Calling the LLM (optional).\n",
    "  - Picking a heuristic fallback.\n",
    "\n",
    "Please:\n",
    "- Generate the full `mysh_llm.py` implementation.\n",
    "- Add short comments explaining the main functions.\n",
    "- Make sure the script can run as:\n",
    "\n",
    "    python mysh_llm.py \"list all C files\"\n",
    "\n",
    "  and print a single suggested command.\n",
    "```\n",
    "\n",
    "You may adapt this prompt with details specific to your environment (e.g., the real path to `mysh`, your catalog format, or your chosen LLM provider).\n",
    "\n",
    "### 10.4 What to hand in\n",
    "\n",
    "For grading, you should provide:\n",
    "- Your `mysh_llm.py` script (and any small helper files it needs).\n",
    "- A short note (markdown or text) describing:\n",
    "  - How you implemented `mysh --commands-json`.\n",
    "  - Whether you used a real LLM or only the heuristic mode.\n",
    "  - One or two example `@` queries and the suggested commands they produced.\n",
    "\n",
    "Optional: include a short reflection on how using an AI assistant changed the way you designed or implemented `mysh_llm.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cf2cf",
   "metadata": {},
   "source": [
    "## 11. Agent MCP Integration – Designing the shell as an AI backend\n",
    "\n",
    "This section treats your shell as a **backend for external agents**, using ideas similar to the Model Context Protocol (MCP).\n",
    "\n",
    "The design goal:\n",
    "- An external agent (running elsewhere, with its own Python/LLM/etc.) can safely and reliably:\n",
    "  - Inspect and edit files.\n",
    "  - Run commands and tests.\n",
    "  - Manage processes.\n",
    "  - Later, use simple networking tools.\n",
    "- The shell provides a **small, well-defined set of capabilities** with structured input/output.\n",
    "- Internally, your shell is BusyBox-like: most commands are built-ins, running in the same process (using `pthread` for background jobs and daemons), not external `exec` calls.\n",
    "\n",
    "From an agent’s point of view, it does not matter whether a command is internal or external, as long as:\n",
    "- The **interface** is stable and discoverable.\n",
    "- Effects are well-defined.\n",
    "- Results are machine-readable when needed.\n",
    "\n",
    "### 11.1 Capabilities an agent needs (minimal toolset)\n",
    "\n",
    "A useful MCP-style integration can be built from a relatively small set of tools. Here is a suggested **minimum set** that also doubles as a checklist for your shell command set.\n",
    "\n",
    "**Filesystem tools**\n",
    "\n",
    "- `fs.list` – list directory contents and basic metadata.\n",
    "  - CLI analogue: `ls`, `find`.\n",
    "  - Shell commands to implement:\n",
    "    - `ls` (with `-a`, `-l`, etc.).\n",
    "    - Optional: a more structured listing command or flag, e.g. `ls --json`.\n",
    "\n",
    "- `fs.read` – read file contents.\n",
    "  - CLI: `cat`, `head`, `tail`.\n",
    "  - Shell commands:\n",
    "    - `cat` (core).\n",
    "    - Optional: `head`, `tail` for partial reads.\n",
    "\n",
    "- `fs.write` / `fs.append` – create/overwrite/append:\n",
    "  - CLI: `cp`, `mv`, `rm`, `mkdir`, `rmdir`, `touch`.\n",
    "  - Shell commands:\n",
    "    - `cp`, `mv`, `rm`, `mkdir`, `rmdir`, `touch`.\n",
    "\n",
    "- `fs.stat` – inspect file type/size/timestamps.\n",
    "  - CLI: `stat`.\n",
    "  - Shell command:\n",
    "    - `stat` (possibly with `--json` output for agents).\n",
    "\n",
    "- `fs.search` – search text in files.\n",
    "  - CLI: `grep` (or `rg` if you mimic ripgrep).\n",
    "  - Shell command:\n",
    "    - `grep` with at least basic regex or fixed-string matching.\n",
    "\n",
    "**Text editing tools**\n",
    "\n",
    "For an agent, powerful but complex tools like `sed` are helpful but not strictly required. Since Python is not guaranteed inside your environment, it is better to design **simple, predictable editing commands** as built-ins.\n",
    "\n",
    "Examples of agent-friendly editing commands:\n",
    "- `edit replace-line FILE N TEXT` – replace line `N` with `TEXT`.\n",
    "- `edit insert-line FILE N TEXT` – insert `TEXT` before line `N`.\n",
    "- `edit delete-line FILE N` – delete line `N`.\n",
    "- `edit replace FILE PATTERN REPLACEMENT` – simple global replacement.\n",
    "\n",
    "These can be implemented as internal commands operating on whole files. For human users, you may still provide `sed`-like tools, but for agents, these structured operations are easier to use safely than arbitrary regex pipelines.\n",
    "\n",
    "**Process and job tools**\n",
    "\n",
    "Your shell already uses `pthread` and internal job management. For an agent, we want tools that:\n",
    "\n",
    "- `proc.list` – show running jobs.\n",
    "  - CLI: `jobs`, `ps` (limited to shell-managed jobs).\n",
    "  - Shell commands:\n",
    "    - `jobs` – list jobs with IDs and state.\n",
    "    - `ps` – optional, to show more detail.\n",
    "\n",
    "- `proc.kill` – terminate or signal a job.\n",
    "  - CLI: `kill`.\n",
    "  - Shell command:\n",
    "    - `kill JOB_ID` or `kill PID`.\n",
    "\n",
    "- `proc.wait` – wait for a job to finish.\n",
    "  - CLI: `wait`.\n",
    "  - Shell command:\n",
    "    - `wait JOB_ID`.\n",
    "\n",
    "**Shell / environment tools**\n",
    "\n",
    "- `cd`, `pwd` – change and inspect current directory.\n",
    "- `env.get` / `env.set` – inspect and change environment variables.\n",
    "  - CLI: `export`, `set`, `unset`.\n",
    "- `which` / `type` – inspect how a command name is resolved (built-in vs external).\n",
    "\n",
    "These are all familiar shell built-ins; the key for MCP is to make their behavior **predictable and scriptable**.\n",
    "\n",
    "**Networking tools (later chapter)**\n",
    "\n",
    "- Minimal HTTP client:\n",
    "  - `http get URL` – fetch content.\n",
    "  - `http post URL DATA` – send data.\n",
    "- Optional: domain-specific tools for your embedded/FreeRTOS environment (e.g., device discovery or configuration).\n",
    "\n",
    "For embedded targets without Python, an external agent (running on a PC) would use these commands via MCP or a serial/network bridge; the shell itself does not need to host the AI.\n",
    "\n",
    "### 11.2 Mapping shell commands to MCP-style tools\n",
    "\n",
    "Conceptually, you can think of an MCP server exposing tools like:\n",
    "\n",
    "- `fs_list(path: string) -> [DirEntry]`\n",
    "- `fs_read(path: string, offset?: int, length?: int) -> string`\n",
    "- `fs_write(path: string, content: string, mode: string) -> void`\n",
    "- `proc_list() -> [JobInfo]`\n",
    "- `proc_kill(id: int, signal: string) -> void`\n",
    "- `http_get(url: string) -> HttpResponse`\n",
    "\n",
    "Inside your shell, these tools do **not** need to be separate programs:\n",
    "\n",
    "- They can be thin wrappers around internal APIs / built-in commands written in C.\n",
    "- For example, `fs_read` might call the same internal function that backs the `cat` command.\n",
    "- `proc_list` might call your job table inspection code that backs `jobs`.\n",
    "\n",
    "Your existing `cmd_spec_t` and registry make it straightforward to:\n",
    "\n",
    "- Discover available commands and their metadata.\n",
    "- Map from a tool name (e.g. `fs_read`) to a handler function.\n",
    "- Eventually generate tool schemas (argument names, types, descriptions) from `argtable3`.\n",
    "\n",
    "### 11.3 No Python inside the shell: what changes?\n",
    "\n",
    "In this design, **Python is not required inside the shell environment**:\n",
    "\n",
    "- The AI agent (and any Python-based RAG code, like `mysh_llm.py`) runs on a host machine or in the cloud.\n",
    "- The shell runs on Linux, FreeRTOS, or other targets, providing only its own commands and internal APIs.\n",
    "- Communication between the agent and the shell can be:\n",
    "  - Local process calls (e.g., running `mysh` with a custom protocol over stdin/stdout).\n",
    "  - Network sockets or serial links to an MCP-style server that wraps the shell.\n",
    "\n",
    "This means:\n",
    "- You **should not** rely on Python scripts for editing files or complex logic inside the embedded environment.\n",
    "- Instead, design a small set of **deterministic, composable built-ins** (like the structured edit commands above) that an external agent can orchestrate.\n",
    "- Tools like `sed` are helpful, but not mandatory; what matters is that the agent can:\n",
    "  - Read a file.\n",
    "  - Make a controlled change.\n",
    "  - Write it back.\n",
    "\n",
    "From the agent’s perspective, the main difference is **where the intelligence runs**:\n",
    "- In the `mysh_llm.py` approach, some logic runs inside the same host OS as the shell.\n",
    "- In an embedded or restricted environment, the AI logic stays outside; it uses MCP or a similar protocol to invoke shell capabilities.\n",
    "\n",
    "Either way, the design principles are the same:\n",
    "- Keep the shell’s command set **complete** and **consistent** for filesystem, processes, and networking.\n",
    "- Provide a **structured catalog** (`--commands-json` and, later, per-command schemas).\n",
    "- Avoid making the agent rely on fragile, human-oriented text parsing when structured outputs are possible.\n",
    "\n",
    "### 11.4 Using this chapter as a requirements list\n",
    "\n",
    "You can use the list above as:\n",
    "- A **requirements checklist** for which built-ins your students implement.\n",
    "- A **design target** for future MCP integration:\n",
    "  - First, make sure each capability exists as a robust command.\n",
    "  - Then, add structured output (`--json` flags, `--commands-json`).\n",
    "  - Later, wrap those capabilities in a dedicated MCP server if desired.\n",
    "\n",
    "This keeps your shell portable (Linux, FreeRTOS, embedded) while still being a first-class backend for AI agents that want to manipulate files, processes, and network resources safely and predictably.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
